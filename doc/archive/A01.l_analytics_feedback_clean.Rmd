---
title: Cleaning of the analytics and feedback data<br><h2>Threatened Australians (threatened.org.au)
author: Gareth Kindler<br>The University of Queensland, Australia
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
    html_document:
        code_folding: show
---

```{r setup, include=FALSE, fig.align='center', warning=FALSE, message=FALSE}
# knitr::opts_chunk$set(echo = TRUE, comment = "#")
# knitr::opts_knit$set(root.dir = "../")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r Libraries, message=FALSE}
library(tidyverse)
library(sf)
library(magrittr)
library(jsonlite)
library(units)
library(readxl)
library(stringr)
library(plotly)
# library(RGoogleAnalytics)
library(forcats)
library(reactable)
library(readODS)
library(naniar)
library(viridis)
library(ggridges)
library(networkD3)
library(maps)
library(vroom)
library(data.table)
```

# Import data

We've got two sources of *data* that were produced by **Threatened Australians**, the first of which is:

## Analytics data

Initially, I was importing this via the predetermined reports (`old` directory in `raw_data`). Then I started to use the *explore* tab on GA4, except this data came with awful messy-ness that wasn't easy to clean on import. Then I tried endlessly to use the API (failed miserably, I think because the Google overlords recently changed the process but was quite happy to provide awful and non-updated documentation on it). I then discovered GA4 only holds data for 2 months. I thus had to export all the data I needed. I gave in to manually changing the tables from the explore tab of GA4 for easier import as figuring out how to do it R was going to be educational but a nightmare (I hate myself). I made a copy of each downloaded file (`/raw_data/raw_downloaded`) and put the edited in the `raw_data` folder.


Acquisition report - how we acquired traffic/users

```{r read-acquisition}
acquis_date <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_acquisition_date_source.ods"
)

acquis_traffic <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_acquisition_traffic_source.ods"
)
```

Engagement report - pages and screens viewed by users. This includes the number of times a specific page was viewed, how many users, how many events (e.g. clicks, scrolls).

```{r read-engagement}
# path_report <- read.csv(
#   "data/TA_analytics/22-05-02_22-06-14_page_path_report.csv",
#   skip = 10020
# )
engag_title <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_engagement_title.ods"
)
engag_title_av_time <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_engagement_page_title.ods"
)
```

Demographics report - cities, regions etc

```{r read-demographics}
demo_city <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_demographics_aus_city.ods"
)

demo_region <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_demographics_aus_region.ods"
)

demo_country <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_demographics_country.ods"
)
```

Technology report - device type people used

There is other information in this report such as dated info.

```{r read-technology}
tech <- read_ods(
  "data/raw_data/TA_analytics/22-05-02_22-06-14_tech_device_category.ods"
)
```

Links - This is the event counts from the links that were clicked on

Due to GA4 holding the data for two months, and my ignorance of this, the links data starts on the 4th of May as opposed to the 2nd of May, like with the other datasets.

```{r read-links}
links <- read_ods(
  "data/raw_data/TA_analytics/22-05-04_22-06-14_links_au_only.ods"
)
```

## Feedback

The other type of data is feedback data - this is a single data frame with the answers to the feedback survey

Export it by copy and pasting the data into a new spreadsheet.

```{r read-feedback}
feedback <- read_ods(
  "data/raw_data/TA_feedback_output/21-to-22-06-16_feedback.ods"
)
```

## Media appearances

```{r}
media <- read_ods(
  "data/raw_data/media.ods"
)
```

## Misc

Other data we are interested in attaching to the analytics and (possibly) the feedback data.

```{r read-misc}
# elects <- fromJSON(
#   "output/clean_data/MP_info_clean.json"
# )
# file.copy(
#     "/home/gareth/science/projects/electoral/threatened_australians/output/clean_data/demo_clean.json",
#     "/home/gareth/science/projects/electoral/threatened_australians_research/data/clean_data/demo_clean.json"
#     )
demo_elects <- fromJSON(
    "data/clean_data/demo_clean.json"
)
```

# Clean

## Analytics

Everything that is needed to clean the analytics data is represented pretty clearly in the code, so I won't elaborate.

```{r functions}
replacePeriodsWithScores <- function(x){
  colnames(x) <- gsub(
    "\\.",
    "\\_",
    colnames(x)
  ); x
}
replaceSpacesWithUnderscores <- function(x){
  colnames(x) <- gsub(
    "\\s",
    "\\_",
    colnames(x)
  ); x
}
```

### Acquisition

```{r acquisition-traffic-clean}
acquis_traffic <- replaceSpacesWithUnderscores(acquis_traffic)
names(acquis_traffic) <- tolower(names(acquis_traffic))
acquis_traffic_clean <- acquis_traffic %>%
  mutate(
    active_users = as.integer(active_users)
  ) %>%
  mutate(
    first_user_source = str_replace(
      .$first_user_source,
      "(direct)",
      "direct"
    )
  ) %T>%
  write_json(
    "data/clean_data/acquis_traffic_clean.json"
  )
```

```{r acquisition-date-clean}
acquis_date <- replacePeriodsWithScores(acquis_date)
names(acquis_date) <- tolower(names(acquis_date))
acquis_date_clean <- acquis_date %>%
  rename(
    date = `session source`,
    direct = `(direct)`,
    abc = abc_net_au,
    theconversation = theconversation_com,
    facebook = m_facebook_com,
    uq = uq_edu_au,
    facebook2 = lm_facebook_com,
    facebook3 = l_facebook_com,
    twitter = t_co,
    reddit = out_reddit_com,
    reddit2 = reddit_com,
    bing = bing,
    google_classroom = classroom_google_com,
    instagram = l_instagram_com,
    linkedin = linkedin_com,
    themandarin = themandarin_com_au,
    we_are_explorers = weareexplorers_co,
    abc2 = `amp-abc-net-au_cdn_ampproject_org`,
    google_docs = docs_google_com,
    bushwalk_updates = `bushwalk updates`
  ) %>%
  mutate(
    date = as.Date(as.character(date), "%Y%m%d")
  ) %>%
  mutate(
    facebook = facebook + facebook2 + facebook3,
    reddit = reddit + reddit2,
    abc = abc + abc2
  ) %>%
  select(
    !c(facebook2, facebook3, reddit2, abc2)
  ) %T>%
  write_json(
    "data/clean_data/acquis_time_clean.json"
  )
```

### Engagement

I chose to use the by title engagement report as opposed to the by path as it was simply cleaner and slightly more informative. Even if the page titles use the vernacular name of species, which isn't distinct like scientific name or their taxon ID.

`engaged_sessions` is probably the most informative metric. Google defines as:

> The number of sessions that lasted 10 seconds or longer, or had 1 or more conversion events or 2 or more page or screen views.

For pages such as "choose your electorate" thi metric doesn't work but we are not interested in this page.

For what all the columns mean, check [this](https://support.google.com/analytics/answer/9143382).

```{r enagagement}
engag_title <- replaceSpacesWithUnderscores(engag_title)
names(engag_title) <- tolower(names(engag_title))
engag_title_clean <- engag_title %>%
  # select(
  #   page_title, active_users, new_users, engaged_sessions, event_count
  # ) %>%
  mutate(
    page_title = str_remove_all(
        .$page_title,
        coll(" - Threatened Australians")
    )
  ) %>%
  mutate(
    page_title = str_replace(
      .$page_title,
      "Threatened Australians - find threatened species near you",
      "Home"
    )
  ) %>%
  filter(
    !str_detect(
      page_title,
      "絶滅の危機に瀕|https://www.threatened.o|wills|ausztrálok"
    )
  ) %T>%
  write_json(
    "data/clean_data/engag_title_clean.json"
  )
```

```{r, message = FALSE}
species_title_clean <- engag_title_clean %>%
  filter(
    !str_detect(
      page_title,
      "Help|Survey|About|Resources|Browse|Choose|Home|Australians"
    )
  ) %>%
  filter(
    !grepl(
      "^Large-eared Pied Bat$",
      page_title
    )
  ) %>%
  mutate(
    vernacular_name = word(
      page_title, 1, sep = fixed(" in ")
    )
  ) %>%
  mutate(
    electorate = word(
      page_title, 2, sep = fixed(" in ")
    )
  ) %>%
  full_join(demo_elects) %>%
  select(
    page_title, electorate, state_territory, state_territory_abbrev, demographic_class, vernacular_name, active_users, new_users, engaged_sessions, engagement_rate, engaged_sessions_per_user, event_count
  ) %T>%
  write_json(
    "data/clean_data/species_title_clean.json"
  )
```

```{r average engagement time}
species_title_av_time_clean <-
  replaceSpacesWithUnderscores(engag_title_av_time)
names(species_title_av_time_clean) <-
  tolower(names(species_title_av_time_clean))
glimpse(species_title_av_time_clean)
ggplot(species_title_av_time_clean) +
  aes(x = average_engagement_time) +
  geom_histogram(binwidth = 1)

```

```{r}
species_title_av_time_clean <- species_title_av_time_clean %>%
  rename(page_title = page_title_and_screen_class) %>%
  select(page_title, average_engagement_time) %>%
  mutate(page_title = str_remove_all(.$page_title,
                                     coll(" - Threatened Australians"))) %>%
  mutate(
    page_title = str_replace(
      .$page_title,
      "Threatened Australians - find threatened species near you",
      "Home"
    )
  ) %>%
  filter(!str_detect(
    page_title,
    "絶滅の危機に瀕|https://www.threatened.o|wills|ausztrálok"
  )) %>%
  filter(
    !str_detect(
      page_title,
      "Help|Survey|About|Resources|Browse|Choose|Home|Australians"
    )
  ) %>%
  filter(!grepl("^Large-eared Pied Bat$",
                page_title)) %>%
  mutate(vernacular_name = word(page_title, 1, sep = fixed(" in "))) %>%
  mutate(electorate = word(page_title, 2, sep = fixed(" in "))) %>%
  full_join(demo_elects) %>%
  group_by(vernacular_name) %>%
  summarise(average_engagement_time = mean(average_engagement_time)) %>%
  arrange(desc(average_engagement_time)) %>%
  mutate(across(average_engagement_time,
                signif,
                digits = 3))
# %T>%
  write_json(
    "data/clean_data/species_title_av_time_clean.json"
  )
```


```{r message = FALSE}
help_title <- engag_title_clean %>%
  filter(
    str_detect(
      page_title,
      "Help"
    )
  ) %>%
  filter(
    !str_detect(
      page_title,
      "wills"
    )
  ) %>%
  mutate(
    electorate = word(
      page_title, 2, sep = fixed(" in ")
    )
  ) %>%
  full_join(., demo_elects) %>%
  relocate(
    page_title, electorate, state_territory, state_territory_abbrev, demographic_class
  ) %T>%
  write_json(
    "data/clean_data/help_title_clean.json"
  )
```

### Demographics

```{r demo-city-clean}
demo_city <- replaceSpacesWithUnderscores(demo_city)
names(demo_city) <- tolower(names(demo_city))
demo_city_clean <- demo_city %>%
  mutate(
    city = str_replace(
      .$city,
      "\\(not set\\)",
      "Unknown"
    )
  )
```

```{r clean global cities data}
world_cities_geom <- world.cities[world.cities$country.etc == "Australia",]
world_cities_geom_clean <- world_cities_geom %>%
  select(name, lat, long) %>%
  rename(city = name) %>%
  st_as_sf(coords = c("long", "lat"), crs = "EPSG:4326")
```

```{r compare and ID empties}
id_empty_cities <- demo_city_clean %>%
  left_join(world_cities_geom_clean) %>%
  st_as_sf(crs = "EPSG:4326")
id_empty_cities <-  st_set_geometry(
    id_empty_cities[st_is_empty(
      id_empty_cities),1], NULL) %>%
  filter(!city == "Unknown")
```

Take those and create a tribble.

```{r fill empties with geoms}
# TODO: These are not WGS84 geoms
cities_empty_with_geom <- tribble(
  ~city, ~lat, ~long,
  'Bathurst', -33.41665, 149.5806,
  'Burnie - Somerset', 41.0597, 145.8937,
  'Central Coast', 33.2320, 151.2173,
  'Drouin', -38.133333, 145.85,
  'Kalgoorlie - Boulder', 30.7490, 121.4660,
  'Kingston', 37.9819, 145.1045,
  'Maitland', 32.7316, 151.5525,
  'Moe - Newborough', 38.1775, 146.2594,
  'Shepparton - Mooroopna', 36.3856, 145.4139
) %>%
  st_as_sf(coords = c("long", "lat"), crs = "EPSG:4326") %>%
  as.data.frame()
```

```{r join}
cities_geom_clean <- world_cities_geom_clean %>%
  full_join(cities_empty_with_geom)
demo_city_geom_clean <- demo_city_clean %>%
  left_join(cities_geom_clean) %>%
  st_as_sf(crs = "EPSG:4326") %T>%
  st_write(
    "data/clean_data/demo_city_geom_clean.gpkg",
    layer = "demo_city_geom_clean", append = FALSE, delete_dsn = TRUE
  )
```

```{r demo-country-clean}
demo_country <- replaceSpacesWithUnderscores(demo_country)
names(demo_country) <- tolower(names(demo_country))
demo_country_clean <- demo_country %T>%
  write_json(
    "data/clean_data/demo_country_clean.json"
  )
```

```{r demo-region-clean}
demo_region <- replaceSpacesWithUnderscores(demo_region)
names(demo_region) <- tolower(names(demo_region))
demo_region_clean <- demo_region %>%
  mutate(
    region = str_replace(
      .$region,
      "\\(not set\\)",
      "Unknown"
    )
  ) %T>%
  write_json(
    "data/clean_data/demo_region_clean.json"
  )
```

### Links

```{r links-clean}
links_clean <- replaceSpacesWithUnderscores(links)
names(links_clean) <- tolower(names(links_clean))
```

```{r links-email-clean}
links_email_clean <- links_clean %>%
  filter(
    str_detect(
      link_url,
      "mailto:"
    )
  ) %>%
  filter(
    !str_detect(
      link_url,
      "mailto:contact@threatened.org.au"
    )
  ) %>%
  mutate(
    link_url_clean = word(
      link_url, 1, 1,
      sep = fixed("?")
    )
  ) %>%
  mutate(
    MP_email = str_replace(
      link_url_clean,
      "mailto:",
      ""
    )
  ) %>%
  select(MP_email, event_count) %>%
  group_by(MP_email) %>%
  summarise(
    event_count = sum(event_count)
  ) %T>%
  write_json(
    "data/clean_data/links_email_clean.json"
  )
```

```{r links-domain-clean}
links_domain_clean <- links_clean %>%
  drop_na(link_domain) %>%
  select(link_domain, link_url, event_count) %>%
  group_by(link_domain) %>%
  summarise(
    event_count = sum(event_count)
  ) %T>%
  write_json(
    "data/clean_data/links_domain_clean.json"
  )
```

## Feedback

Cleaning the feedback data was pretty easy besides not being able to figure out why I couldn't convert the blankspaces to `NA`s. Felt like I tried everything, so I gave up. If I want to pick this back up, start with trying to extract the regex characters of the missing values as doing it the other way around wasn't working. A work around for the multiple choice questions was to set the levels for the factor and anything left over (the blanks) get converted to `NA`s.

```{r feedback-clean}
feedback_clean <- feedback %>%
  rename(
    submission_id = submissionid,
    key_takeaway = answer1,
    actions_result = answer2,
    actions_be = answer3,
    easy_to_understand = answer4,
    improvments_suggestions = answer5,
    anything_else = answer6
  ) %>%
  filter(
    !str_detect(
      key_takeaway,
      "test|Test|выплат|готовы")
  ) %>%
  filter(
    !submission_id %in% c(1:11, 47:49)
  ) %T>%
  write_json(
    "data/clean_data/feedback_clean.json"
  )
```

After removing test submissions of the feedback form and some dodgy answers, we went from `r dim(feedback)[1]` to `r dim(feedback_clean)[1]`.

## Media

```{r}
media_clean <- media %T>%
  write_json(
    "data/clean_data/media_clean.json"
  )
```
```